{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####                                                                        GPT2 Fine-tuning ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "from logging import config\n",
    "from random import shuffle\n",
    "import tokenize\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import os \n",
    "import nltk \n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from colorama import Style, Fore\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_spam_data(debug = False, batch_size = 2):\n",
    "    df = pd.read_csv(os.path.join(\"dataset\", \"spam.csv\"), encoding='latin1')\n",
    "    df_ham = df[df['v1'] == 'ham'][['v2']]\n",
    "    '''\n",
    "    It handles punctuation intelligently (e.g., separating punctuation from words).\n",
    "    It accounts for contractions (e.g., \"don't\" is split into [\"do\", \"n't\"]).\n",
    "    '''\n",
    "    # df_ham['sentence'] = df_ham['v2'].apply(lambda sentence: nltk.word_tokenize(sentence))\n",
    "    df_ham['sentence'] = df_ham['v2']\n",
    "    df_ham.drop(columns = ['v2'], axis = 1, inplace = True)\n",
    "    df_ham['length'] = df_ham['sentence'].apply(lambda x: len(x))\n",
    "    if debug:\n",
    "        sns.histplot(data = df_ham, x = 'length', kde = True)\n",
    "        plt.xlabel('Length of sentence.')\n",
    "        plt.savefig(os.path.join('plots', 'SentenceLength.png'))\n",
    "        plt.show()\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, txt_list, tokenizer, max_length = 128):\n",
    "            self.input_ids, self.attn_masks = [], []\n",
    "            for txt in txt_list:\n",
    "                encoding_dict = tokenizer('<|start|>' + txt + '<|end|>', \\\n",
    "                                          truncation = True, \\\n",
    "                                          padding = 'max_length', \\\n",
    "                                          max_length = max_length)\n",
    "                self.input_ids.append(encoding_dict['input_ids'])\n",
    "                self.attn_masks.append(encoding_dict['attention_mask'])\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.input_ids[idx], self.attn_masks[idx]\n",
    "    '''\n",
    "        In GPT-2, the default value for tokenizer.bos_token_id is None \n",
    "        because GPT-2 does not use a beginning-of-sequence (BOS) token by default.\n",
    "    '''\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|start|>', \\\n",
    "                                              eos_token='<|end|>', pad_token='<|pad|>')\n",
    "    print(f'{Fore.CYAN}[TOKNIZR] Mx model length: {tokenizer.model_max_length}. '+\\\n",
    "          f'GPT small: 768{Style.RESET_ALL}')\n",
    "    print(f'{Fore.CYAN}[TOKNIZR] Beginning of seq: {tokenizer.decode(tokenizer.bos_token_id)}, '+\\\n",
    "          f'has token id: {tokenizer.bos_token_id}{Style.RESET_ALL}')\n",
    "    print(f'{Fore.CYAN}[TOKNIZR] End of seq: {tokenizer.decode(tokenizer.eos_token_id)}, '+\\\n",
    "          f'has token id: {tokenizer.eos_token_id}{Style.RESET_ALL}')\n",
    "    print(f'{Fore.CYAN}[TOKNIZR] Padding of seq: {tokenizer.decode(tokenizer.pad_token_id)}, '+\\\n",
    "          f'has token id: {tokenizer.pad_token_id}{Style.RESET_ALL}')\n",
    "    dataset = CustomDataset(df_ham['sentence'], tokenizer)\n",
    "    tr_size = int(0.95 * len(dataset))\n",
    "    train_dataset, validation_dataset = random_split(dataset, [tr_size, len(dataset) - tr_size])\n",
    "    train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size)\n",
    "    validation_loader = DataLoader(validation_dataset, shuffle = False, batch_size = batch_size)\n",
    "    return train_loader, validation_loader, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, train_loader, validation_loader):\n",
    "    epochs = 5\n",
    "    learning_rate = 5e-4\n",
    "    warmup_steps = 1e2\n",
    "    epsilon = 1e-8\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    optimizer = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)\n",
    "    '''\n",
    "        1. Gradually increases the learning rate from 0 to the initial maximum value over num_warmup_step\n",
    "           This prevents abrupt large updates at the start of training when weights are randomly initialized.\n",
    "        2. After the warm-up steps, the learning rate linearly decreases to 0 over the remaining num_training_steps\n",
    "    '''\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps = warmup_steps,\n",
    "                                                num_training_steps = total_steps)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Selects device\n",
    "    model = model.to(device)\n",
    "    tbegin = time.time()\n",
    "    stats = {\"training_loss\": [], \"validation_loss\": []}\n",
    "    for epoch in range(epochs):\n",
    "        print(f'======== Epoch {1+epoch} / {epochs} ========')\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            token = torch.stack(batch[0], dim = 0).permute(1, 0)\n",
    "            attn = torch.stack(batch[1], dim = 0).permute(1, 0)\n",
    "            b_input_ids = token.to(device)\n",
    "            b_labels = token.to(device)\n",
    "            b_masks = attn.to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, labels = b_labels, \\\n",
    "                            attention_mask = b_masks, token_type_ids = None)\n",
    "            loss = outputs.loss \n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_loss = total_train_loss / len(train_loader)\n",
    "        print(f'Time: {round(time.time() - t0, 3)} sec, Avg Loss: {round(avg_loss, 4)}')\n",
    "        stats[\"training_loss\"].append(avg_loss)\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        t0 = time.time()\n",
    "        total_val_loss = 0\n",
    "        model.eval()\n",
    "        for batch in validation_loader:\n",
    "            token = torch.stack(batch[0], dim = 0).permute(1, 0)\n",
    "            attn = torch.stack(batch[1], dim = 0).permute(1, 0)\n",
    "            b_input_ids = token.to(device)\n",
    "            b_labels = token.to(device)\n",
    "            b_masks = attn.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, labels = b_labels, \\\n",
    "                                attention_mask = b_masks, token_type_ids = None)\n",
    "                total_val_loss += outputs.loss.item()\n",
    "        avg_loss = total_val_loss / len(validation_loader)\n",
    "        print(f'Time: {round(time.time() - t0, 3)} sec, Avg Loss: {round(avg_loss, 4)}')\n",
    "        stats[\"validation_loss\"].append(avg_loss)\n",
    "    print(f'------ Training Completed ------ {round(time.time() - tbegin), 3} sec.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[TOKNIZR] Mx model length: 1024. GPT small: 768\u001b[0m\n",
      "\u001b[36m[TOKNIZR] Beginning of seq: <|start|>, has token id: 50257\u001b[0m\n",
      "\u001b[36m[TOKNIZR] End of seq: <|end|>, has token id: 50258\u001b[0m\n",
      "\u001b[36m[TOKNIZR] Padding of seq: <|pad|>, has token id: 50259\u001b[0m\n",
      "\u001b[32m[GPT2]:\n",
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 768)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, validation_loader, tokenizer = prepare_spam_data()\n",
    "configurations = GPT2Config.from_pretrained('gpt2', output_hidden_states = False)\n",
    "print(f'{Fore.GREEN}[GPT2]:\\n{configurations}{Style.RESET_ALL}')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', config = configurations)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 5 ========\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#               Fine Tune\n",
    "# ========================================\n",
    "finetune(model, train_loader, validation_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
