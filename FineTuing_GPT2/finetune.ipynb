{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "from logging import config\n",
    "from random import shuffle\n",
    "import tokenize\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import os \n",
    "import nltk \n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from colorama import Style, Fore\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_spam_data(debug = False, batch_size = 2):\n",
    "    df = pd.read_csv(os.path.join(\"dataset\", \"spam.csv\"), encoding='latin1')\n",
    "    df_ham = df[df['v1'] == 'ham'][['v2']]\n",
    "    '''\n",
    "    It handles punctuation intelligently (e.g., separating punctuation from words).\n",
    "    It accounts for contractions (e.g., \"don't\" is split into [\"do\", \"n't\"]).\n",
    "    '''\n",
    "    # df_ham['sentence'] = df_ham['v2'].apply(lambda sentence: nltk.word_tokenize(sentence))\n",
    "    df_ham['sentence'] = df_ham['v2']\n",
    "    df_ham.drop(columns = ['v2'], axis = 1, inplace = True)\n",
    "    df_ham['length'] = df_ham['sentence'].apply(lambda x: len(x))\n",
    "    if debug:\n",
    "        sns.histplot(data = df_ham, x = 'length', kde = True)\n",
    "        plt.xlabel('Length of sentence.')\n",
    "        plt.savefig(os.path.join('plots', 'SentenceLength.png'))\n",
    "        plt.show()\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, txt_list, tokenizer, max_length = 128):\n",
    "            self.input_ids, self.attn_masks = [], []\n",
    "            for txt in txt_list:\n",
    "                encoding_dict = tokenizer('<|start|>' + txt + '<|end|>', \\\n",
    "                                          truncation = True, \\\n",
    "                                          padding = 'max_length', \\\n",
    "                                          max_length = max_length)\n",
    "                self.input_ids.append(encoding_dict['input_ids'])\n",
    "                self.attn_masks.append(encoding_dict['attention_mask'])\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.input_ids[idx], self.attn_masks[idx]\n",
    "    '''\n",
    "        In GPT-2, the default value for tokenizer.bos_token_id is None \n",
    "        because GPT-2 does not use a beginning-of-sequence (BOS) token by default.\n",
    "    '''\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|start|>', \\\n",
    "                                              eos_token='<|end|>', pad_token='<|pad|>')\n",
    "    print(f'{Fore.CYAN}[TOKNIZR] Mx model length: {tokenizer.model_max_length}. '+\\\n",
    "          f'GPT small: 768{Style.RESET_ALL}')\n",
    "    print(f'{Fore.CYAN}[TOKNIZR] Beginning of seq: {tokenizer.decode(tokenizer.bos_token_id)}, '+\\\n",
    "          f'has token id: {tokenizer.bos_token_id}{Style.RESET_ALL}')\n",
    "    print(f'{Fore.CYAN}[TOKNIZR] End of seq: {tokenizer.decode(tokenizer.eos_token_id)}, '+\\\n",
    "          f'has token id: {tokenizer.eos_token_id}{Style.RESET_ALL}')\n",
    "    print(f'{Fore.CYAN}[TOKNIZR] Padding of seq: {tokenizer.decode(tokenizer.pad_token_id)}, '+\\\n",
    "          f'has token id: {tokenizer.pad_token_id}{Style.RESET_ALL}')\n",
    "    dataset = CustomDataset(df_ham['sentence'], tokenizer)\n",
    "    tr_size = int(0.95 * len(dataset))\n",
    "    train_dataset, validation_dataset = random_split(dataset, [tr_size, len(dataset) - tr_size])\n",
    "    train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size)\n",
    "    validation_loader = DataLoader(validation_dataset, shuffle = False, batch_size = batch_size)\n",
    "    return train_loader, validation_loader, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, train_loader, validation_loader):\n",
    "    epochs = 5\n",
    "    learning_rate = 5e-4\n",
    "    warmup_steps = 1e2\n",
    "    epsilon = 1e-8\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    optimizer = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)\n",
    "    '''\n",
    "        1. Gradually increases the learning rate from 0 to the initial maximum value over num_warmup_step\n",
    "           This prevents abrupt large updates at the start of training when weights are randomly initialized.\n",
    "        2. After the warm-up steps, the learning rate linearly decreases to 0 over the remaining num_training_steps\n",
    "    '''\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps = warmup_steps,\n",
    "                                                num_training_steps = total_steps)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Selects device\n",
    "    model = model.to(device)\n",
    "    tbegin = time.time()\n",
    "    stats = {\"training_loss\": [], \"validation_loss\": []}\n",
    "    for epoch in range(epochs):\n",
    "        print(f'======== Epoch {1+epoch} / {epochs} ========')\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            token = torch.stack(batch[0], dim = 0).permute(1, 0)\n",
    "            attn = torch.stack(batch[1], dim = 0).permute(1, 0)\n",
    "            b_input_ids = token.to(device)\n",
    "            b_labels = token.to(device)\n",
    "            b_masks = attn.to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, labels = b_labels, \\\n",
    "                            attention_mask = b_masks, token_type_ids = None)\n",
    "            loss = outputs.loss \n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_loss = total_train_loss / len(train_loader)\n",
    "        print(f'Time: {round(time.time() - t0, 3)} sec, Avg Loss: {round(avg_loss, 4)}')\n",
    "        stats[\"training_loss\"].append(avg_loss)\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        t0 = time.time()\n",
    "        total_val_loss = 0\n",
    "        model.eval()\n",
    "        for batch in validation_loader:\n",
    "            token = torch.stack(batch[0], dim = 0).permute(1, 0)\n",
    "            attn = torch.stack(batch[1], dim = 0).permute(1, 0)\n",
    "            b_input_ids = token.to(device)\n",
    "            b_labels = token.to(device)\n",
    "            b_masks = attn.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, labels = b_labels, \\\n",
    "                                attention_mask = b_masks, token_type_ids = None)\n",
    "                total_val_loss += outputs.loss.item()\n",
    "        avg_loss = total_val_loss / len(validation_loader)\n",
    "        print(f'Time: {round(time.time() - t0, 3)} sec, Avg Loss: {round(avg_loss, 4)}')\n",
    "        stats[\"validation_loss\"].append(avg_loss)\n",
    "    print(f'------ Training Completed ------ {round(time.time() - tbegin), 3} sec.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[TOKNIZR] Mx model length: 1024. GPT small: 768\u001b[0m\n",
      "\u001b[36m[TOKNIZR] Beginning of seq: <|start|>, has token id: 50257\u001b[0m\n",
      "\u001b[36m[TOKNIZR] End of seq: <|end|>, has token id: 50258\u001b[0m\n",
      "\u001b[36m[TOKNIZR] Padding of seq: <|pad|>, has token id: 50259\u001b[0m\n",
      "\u001b[32m[GPT2]:\n",
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 768)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, validation_loader, tokenizer = prepare_spam_data()\n",
    "configurations = GPT2Config.from_pretrained('gpt2', output_hidden_states = False)\n",
    "print(f'{Fore.GREEN}[GPT2]:\\n{configurations}{Style.RESET_ALL}')\n",
    "'''\n",
    "When you load a model using GPT2LMHeadModel.from_pretrained('gpt2', config=configurations), the model weights are not automatically saved to disk. \n",
    "Instead, the model is loaded into memory and available as a Python object (model). \n",
    "The source of the weights depends on the context:\n",
    "'''\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', config = configurations)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 5 ========\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#               Fine Tune\n",
    "# ========================================\n",
    "finetune(model, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display statistics & model info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid')\n",
    "stats_df = pd.DataFrame(stats)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data = stats_df, x = range(stats_df.shape[0]), y = 'training_loss', \\\n",
    "             label = 'training_loss', color = 'blue', marker = 'o')\n",
    "sns.lineplot(data = stats_df, x = range(stats_df.shape[0]), y = 'validation_loss', \\\n",
    "             label = 'validation_loss', color = 'red', marker = 's')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())\n",
    "print(f'GPT2 model has {len(params)} named parameters.')\n",
    "print('\\n============ Embedding Layer ============')\n",
    "'''\n",
    "This is the word embedding table.\n",
    "\n",
    "Shape:\n",
    "\n",
    "50257: The size of the vocabulary. This number corresponds to the unique tokens (words, subwords, punctuation, etc.) that the model can recognize.\n",
    "768: The dimensionality of the embedding space. Each token is represented as a 768-dimensional vector.\n",
    "Purpose:\n",
    "\n",
    "Maps token indices (integers) into dense vectors of size 768.\n",
    "These vectors capture semantic and syntactic information about the tokens.\n",
    "'''\n",
    "'''\n",
    "This is the positional embedding table.\n",
    "\n",
    "Shape:\n",
    "\n",
    "1024: The maximum sequence length that the model can handle. The model can process sequences up to 1024 tokens long.\n",
    "768: The dimensionality of the embedding space, matching the word embedding size.\n",
    "Purpose:\n",
    "\n",
    "Provides positional information to the model by assigning a unique embedding to each position in the sequence.\n",
    "Since Transformers lack inherent sequence ordering (unlike RNNs), positional embeddings are added to the word embeddings to encode the order of tokens.\n",
    "'''\n",
    "for p in params[:2]:\n",
    "    print(f'{p[0]} {str(tuple(p[1].size()))}')\n",
    "print('\\n============ First Transformer ============')\n",
    "for p in params[2:14]:\n",
    "    print(f'{p[0]} {str(tuple(p[1].size()))}')\n",
    "print('\\n============ Output Layer ============')\n",
    "for p in params[-2:]:\n",
    "    print(f'{p[0]} {str(tuple(p[1].size()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & loading fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "output_dir = './save_model/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l --block-size=M ./save_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(os.path.join('./save_model/'))\n",
    "configuration = GPT2Config.from_pretrained(os.path.join('./save_model/', 'config.json'), output_hidden_states=False)\n",
    "model = GPT2LMHeadModel.from_pretrained(os.path.join('./save_model/', 'pytorch_model.bin'), config = configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== [0] ====:\n",
      " The sun will not shine through this window, and it will never be bright enough for me, and for everyone who watches me, I will be lost forever.\n",
      "\n",
      "The sun will not shine through this window, and it will never be bright enough for me, and for everyone who watches me, I will be lost forever. The world will never stop.\n",
      "\n",
      "The world will never stop. I have been told that I have been chosen, that I can not be saved.\n",
      "\n",
      "I have been told that I can not be saved. I am told that it is impossible for me to save this world.\n",
      "\n",
      "I am\n",
      "==== [1] ====:\n",
      " The sun will not shine on this land, and there will be no moon to look at.\" (L. G. Wells, On the Origin of the Bible, p. 394)\n",
      "\n",
      "\"The earth is in a state of decay, and the sky is falling down, and there will be no moon to look at. It will be like a cloud in a lake, which will fall, and the sun will not shine on it.\" (L. G. Wells, On the Origin of the Bible, p. 398)\n",
      "\n",
      "\"The earth will be in a state of decay, and there will be no moon to\n",
      "==== [2] ====:\n",
      " The sun will not shine through. It will shine through with the light. But there are many things that we cannot see. The sun will not shine through.\n",
      "\n",
      "But the moon will shine through with light. The sun is the first light in the universe. It is light that we see in the world. And we are not the first people that see the light of the Sun. But there are many things that we cannot see. The moon will shine through with light. The sun is the first light in the universe. It is light that we see in the world. And we are not the first people that see the light of\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "prompt = \"The sun will not shine\"\n",
    "prompt_tokens = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "'''\n",
    "1. do_sample = True\n",
    "Purpose: Enables sampling instead of deterministic (greedy or beam search) decoding.\n",
    "Effect: The model samples tokens from the probability distribution, introducing randomness. This is essential for generating diverse and creative outputs.\n",
    "Use case: Creative tasks like story generation or poetry, where multiple plausible continuations are possible.\n",
    "\n",
    "2. top_k = 10\n",
    "Purpose: Activates Top-K sampling, which limits the model to consider only the top k tokens with the highest probabilities for the next word.\n",
    "Effect: Prevents low-probability (and often nonsensical) tokens from being sampled.\n",
    "Value (10): The model will choose the next token from the top 50 most probable tokens.\n",
    "\n",
    "3. max_length = 128\n",
    "Purpose: Sets the maximum length of the generated sequence, including the input prompt.\n",
    "Effect: Ensures that the output sequence does not exceed 300 tokens. This is useful to control the length of the output for applications like summaries, responses, or articles.\n",
    "Caution: If the model reaches max_length without completing a meaningful sequence, the output might feel truncated unless eos_token_id is defined.\n",
    "\n",
    "4. top_p = 0.95\n",
    "Purpose: Activates Top-P (nucleus) sampling. Instead of selecting tokens based solely on top_k, it considers the smallest set of tokens whose cumulative probability exceeds top_p.\n",
    "Effect: Combines flexibility and control, ensuring the model samples from high-probability tokens while allowing more diversity than strict Top-K sampling.\n",
    "Value (0.95): Tokens are sampled until their cumulative probability is 95%, allowing for diversity without overly random choices.\n",
    "\n",
    "5. num_return_sequences = 3\n",
    "Purpose: Specifies the number of different sequences to generate for the same input.\n",
    "Effect: Produces multiple outputs, which can be useful for selecting the best one or exploring different plausible continuations.\n",
    "Value (3): Generates 3 distinct sequences.\n",
    "'''\n",
    "sample_outputs = model.generate(prompt_tokens,\\\n",
    "                                do_sample = True,\\\n",
    "                                top_k = 10,\\\n",
    "                                max_length = 128,\\\n",
    "                                top_p = 0.95,\\\n",
    "                                num_return_sequences = 3)\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f'==== [{i}] ====:\\n {tokenizer.decode(sample_output, skip_special_tokens=True)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
