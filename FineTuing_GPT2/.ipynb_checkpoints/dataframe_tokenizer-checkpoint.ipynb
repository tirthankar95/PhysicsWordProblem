{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/spam.csv\", encoding = 'latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>5560</th>\n",
       "      <th>5561</th>\n",
       "      <th>5562</th>\n",
       "      <th>5563</th>\n",
       "      <th>5564</th>\n",
       "      <th>5565</th>\n",
       "      <th>5568</th>\n",
       "      <th>5569</th>\n",
       "      <th>5570</th>\n",
       "      <th>5571</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>v2</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "      <td>...</td>\n",
       "      <td>Anything lor. Juz both of us lor.</td>\n",
       "      <td>Get me out of this dump heap. My mom decided t...</td>\n",
       "      <td>Ok lor... Sony ericsson salesman... I ask shuh...</td>\n",
       "      <td>Ard 6 like dat lor.</td>\n",
       "      <td>Why don't you wait 'til at least wednesday to ...</td>\n",
       "      <td>Huh y lei...</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 4825 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0     \\\n",
       "v2  Go until jurong point, crazy.. Available only ...   \n",
       "\n",
       "                             1     \\\n",
       "v2  Ok lar... Joking wif u oni...   \n",
       "\n",
       "                                                 3     \\\n",
       "v2  U dun say so early hor... U c already then say...   \n",
       "\n",
       "                                                 4     \\\n",
       "v2  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                                 6     \\\n",
       "v2  Even my brother is not like to speak with me. ...   \n",
       "\n",
       "                                                 7     \\\n",
       "v2  As per your request 'Melle Melle (Oru Minnamin...   \n",
       "\n",
       "                                                 10    \\\n",
       "v2  I'm gonna be home soon and i don't want to tal...   \n",
       "\n",
       "                                                 13    \\\n",
       "v2  I've been searching for the right words to tha...   \n",
       "\n",
       "                                   14                          16    ...  \\\n",
       "v2  I HAVE A DATE ON SUNDAY WITH WILL!!  Oh k...i'm watching here:)  ...   \n",
       "\n",
       "                                 5560  \\\n",
       "v2  Anything lor. Juz both of us lor.   \n",
       "\n",
       "                                                 5561  \\\n",
       "v2  Get me out of this dump heap. My mom decided t...   \n",
       "\n",
       "                                                 5562                 5563  \\\n",
       "v2  Ok lor... Sony ericsson salesman... I ask shuh...  Ard 6 like dat lor.   \n",
       "\n",
       "                                                 5564          5565  \\\n",
       "v2  Why don't you wait 'til at least wednesday to ...  Huh y lei...   \n",
       "\n",
       "                                     5568  \\\n",
       "v2  Will Ì_ b going to esplanade fr home?   \n",
       "\n",
       "                                                 5569  \\\n",
       "v2  Pity, * was in mood for that. So...any other s...   \n",
       "\n",
       "                                                 5570  \\\n",
       "v2  The guy did some bitching but I acted like i'd...   \n",
       "\n",
       "                          5571  \n",
       "v2  Rofl. Its true to its name  \n",
       "\n",
       "[1 rows x 4825 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ham = df[df['v1'] == 'ham'][['v2']]\n",
    "df_ham.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5572, 5), (4825, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_ham.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/tirthankar-mittra/ai_ml/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/tirthankar-mittra/ai_ml/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/tirthankar-mittra/ai_ml/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/tirthankar-mittra/ai_ml/lib/python3.12/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /home/tirthankar-mittra/ai_ml/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It handles punctuation intelligently (e.g., separating punctuation from words).\n",
    "It accounts for contractions (e.g., \"don't\" is split into [\"do\", \"n't\"])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Go, until, jurong, point, ,, crazy, .., Avail...\n",
       "1                [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
       "3       [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
       "4       [Nah, I, do, n't, think, he, goes, to, usf, ,,...\n",
       "6       [Even, my, brother, is, not, like, to, speak, ...\n",
       "                              ...                        \n",
       "5565                                   [Huh, y, lei, ...]\n",
       "5568     [Will, Ì_, b, going, to, esplanade, fr, home, ?]\n",
       "5569    [Pity, ,, *, was, in, mood, for, that, ., So, ...\n",
       "5570    [The, guy, did, some, bitching, but, I, acted,...\n",
       "5571                  [Rofl, ., Its, true, to, its, name]\n",
       "Name: v2, Length: 4825, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "df_ham['v2'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'name', 'is', 'Tirthankar', 'Mittra', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = \"My name is Tirthankar Mittra.\"\n",
    "nltk.word_tokenize(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3666,  1438,   318,   309,  3333,   962,   283, 16627,   430,    13]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer \n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', \n",
    "                                          bos_token='<|startoftext|>', \\\n",
    "                                          eos_token='<|endoftext|>', \\\n",
    "                                          pad_token='<|pad|>')\n",
    "text = \"My name is Tirthankar Mittra.\"\n",
    "tokenizer.encode(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [50257, 3666, 1438, 318, 309, 3333, 962, 283, 16627, 430, 13, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [50257, 464, 3797, 318, 2491, 319, 262, 2603, 20023, 262, 4227, 13, 50256, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "text1 = \"My name is Tirthankar Mittra.\"\n",
    "print(tokenizer('<|startoftext|>' + text1 + '<|endoftext|>', \\\n",
    "                truncation = True, \\\n",
    "                padding = 'max_length', \\\n",
    "                max_length = 12))\n",
    "\n",
    "text2 = \"The cat is running on the mat chasing the rat.\"\n",
    "print(tokenizer('<|startoftext|>' + text2 + '<|endoftext|>', \\\n",
    "                truncation = True, \\\n",
    "                padding = 'max_length', \\\n",
    "                max_length = 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================\n",
    "#               Load model\n",
    "# ========================================\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text (Novelty, non-deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "prompt = \"The Cosmos\"\n",
    "prompt_tokens = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "'''\n",
    "1. do_sample = True\n",
    "Purpose: Enables sampling instead of deterministic (greedy or beam search) decoding.\n",
    "Effect: The model samples tokens from the probability distribution, introducing randomness. This is essential for generating diverse and creative outputs.\n",
    "Use case: Creative tasks like story generation or poetry, where multiple plausible continuations are possible.\n",
    "\n",
    "2. top_k = 10\n",
    "Purpose: Activates Top-K sampling, which limits the model to consider only the top k tokens with the highest probabilities for the next word.\n",
    "Effect: Prevents low-probability (and often nonsensical) tokens from being sampled.\n",
    "Value (10): The model will choose the next token from the top 50 most probable tokens.\n",
    "\n",
    "3. max_length = 128\n",
    "Purpose: Sets the maximum length of the generated sequence, including the input prompt.\n",
    "Effect: Ensures that the output sequence does not exceed 300 tokens. This is useful to control the length of the output for applications like summaries, responses, or articles.\n",
    "Caution: If the model reaches max_length without completing a meaningful sequence, the output might feel truncated unless eos_token_id is defined.\n",
    "\n",
    "4. top_p = 0.95\n",
    "Purpose: Activates Top-P (nucleus) sampling. Instead of selecting tokens based solely on top_k, it considers the smallest set of tokens whose cumulative probability exceeds top_p.\n",
    "Effect: Combines flexibility and control, ensuring the model samples from high-probability tokens while allowing more diversity than strict Top-K sampling.\n",
    "Value (0.95): Tokens are sampled until their cumulative probability is 95%, allowing for diversity without overly random choices.\n",
    "\n",
    "5. num_return_sequences = 3\n",
    "Purpose: Specifies the number of different sequences to generate for the same input.\n",
    "Effect: Produces multiple outputs, which can be useful for selecting the best one or exploring different plausible continuations.\n",
    "Value (3): Generates 3 distinct sequences.\n",
    "'''\n",
    "sample_outputs = model.generate(prompt_tokens,\\\n",
    "                                do_sample = True,\\\n",
    "                                top_k = 10,\\\n",
    "                                max_length = 128,\\\n",
    "                                top_p = 0.95,\\\n",
    "                                num_return_sequences = 3)\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f'==== [{i}] ====:\\n {tokenizer.decode(sample_output, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEAM Search (Deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== [0] ====:\n",
      " The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "\n",
      "The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "\n",
      "The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "\n",
      "The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "\n",
      "The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "\n",
      "The Cosmos are in the midst of a three-game homestand against the New York\n",
      "==== [1] ====:\n",
      " The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "\n",
      "The Cosmos are in the midst of a three-game homestand against the New York Cosmos. The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "\n",
      "The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "\n",
      "The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "\n",
      "The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "==== [2] ====:\n",
      " The Cosmos are in the midst of a three-game homestand against the New York Cosmos.\n",
      "\n",
      "The Cosmos are in the midst of a three-game homestand against the New York Cosmos. The Cosmos are in the midst of a three-game homestand against the New York Cosmos. The Cosmos are in the midst of a three-game homestand against the New York Cosmos. The Cosmos are in the midst of a three-game homestand against the New York Cosmos. The Cosmos are in the midst of a three-game homestand against the New York Cosmos. The Cosmos are in the midst\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "prompt = \"The Cosmos\"\n",
    "prompt_tokens = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "'''\n",
    "1. do_sample = True\n",
    "Purpose: Enables sampling instead of deterministic (greedy or beam search) decoding.\n",
    "Effect: The model samples tokens from the probability distribution, introducing randomness. This is essential for generating diverse and creative outputs.\n",
    "Use case: Creative tasks like story generation or poetry, where multiple plausible continuations are possible.\n",
    "\n",
    "2. top_k = 10\n",
    "Purpose: Activates Top-K sampling, which limits the model to consider only the top k tokens with the highest probabilities for the next word.\n",
    "Effect: Prevents low-probability (and often nonsensical) tokens from being sampled.\n",
    "Value (10): The model will choose the next token from the top 50 most probable tokens.\n",
    "\n",
    "3. max_length = 128\n",
    "Purpose: Sets the maximum length of the generated sequence, including the input prompt.\n",
    "Effect: Ensures that the output sequence does not exceed 300 tokens. This is useful to control the length of the output for applications like summaries, responses, or articles.\n",
    "Caution: If the model reaches max_length without completing a meaningful sequence, the output might feel truncated unless eos_token_id is defined.\n",
    "\n",
    "4. top_p = 0.95\n",
    "Purpose: Activates Top-P (nucleus) sampling. Instead of selecting tokens based solely on top_k, it considers the smallest set of tokens whose cumulative probability exceeds top_p.\n",
    "Effect: Combines flexibility and control, ensuring the model samples from high-probability tokens while allowing more diversity than strict Top-K sampling.\n",
    "Value (0.95): Tokens are sampled until their cumulative probability is 95%, allowing for diversity without overly random choices.\n",
    "\n",
    "5. num_return_sequences = 3\n",
    "Purpose: Specifies the number of different sequences to generate for the same input.\n",
    "Effect: Produces multiple outputs, which can be useful for selecting the best one or exploring different plausible continuations.\n",
    "Value (3): Generates 3 distinct sequences.\n",
    "'''\n",
    "## BEAM search is deterministic, top_k, top_p doesn't matter.\n",
    "sample_outputs = model.generate(prompt_tokens,\\\n",
    "                                max_length = 128,\\\n",
    "                                num_beams=5,\n",
    "                                num_return_sequences = 3)\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f'==== [{i}] ====:\\n {tokenizer.decode(sample_output, skip_special_tokens=True)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
